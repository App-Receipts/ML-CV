{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "279dc0a67839aaef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from glob import glob\n",
    "from config import CFG"
   ],
   "id": "27e4632f3bab7249",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "IMG_FILES = glob(CFG.img_path + '/*.jpg')\n",
    "ANN_FILES = glob(CFG.ann_path + '/*.xml')\n",
    "len(IMG_FILES), len(ANN_FILES)"
   ],
   "id": "73b724e55213483a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from data import build_df",
   "id": "10c9e355bf26b5c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df, classes = build_df(ANN_FILES)\n",
    "cls2id = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "id2cls = {i: cls_name for i, cls_name in enumerate(classes)}\n",
    "\n",
    "print(len(classes))\n",
    "df.head()"
   ],
   "id": "3f4da7193afab888",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from data import split_df",
   "id": "58afce8f28a9258a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_df, valid_df = split_df(df)\n",
    "print(\"Train size: \", train_df['id'].nunique())\n",
    "print(\"Valid size: \", valid_df['id'].nunique())"
   ],
   "id": "4a581139f297c2c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from tokenizer import Tokenizer",
   "id": "be95e32e0cadc0e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer(num_classes=len(classes), num_bins=CFG.num_bins, width=CFG.img_size, height=CFG.img_size, max_len=CFG.max_len)\n",
    "CFG.pad_idx = tokenizer.PAD_code"
   ],
   "id": "53f6d1e6da2c4653",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from data import get_loaders",
   "id": "704a63263285adb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_loader, valid_loader = get_loaders(train_df, valid_df, tokenizer, CFG.img_size, CFG.batch_size, CFG.max_len, tokenizer.PAD_code)",
   "id": "4ab97c398f1eb33e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from models import Encoder, Decoder, EncoderDecoder",
   "id": "46acede820c5fc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "encoder = Encoder(model_name=CFG.model_name, pretrained=True, out_dim=256)\n",
    "decoder = Decoder(vocab_size=tokenizer.vocab_size,\n",
    "                  encoder_length=CFG.num_patches, dim=256, num_heads=8, num_layers=6)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "model.to(CFG.device);"
   ],
   "id": "c4f7273dd2afb02d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from train_eval import train_eval"
   ],
   "id": "dbd27b498b426e2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "\n",
    "num_training_steps = CFG.epochs * (len(train_loader.dataset) // CFG.batch_size)\n",
    "num_warmup_steps = int(0.05 * num_training_steps)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                               num_training_steps=num_training_steps,\n",
    "                                               num_warmup_steps=num_warmup_steps)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=CFG.pad_idx)\n",
    "\n",
    "train_eval(model,\n",
    "           train_loader,\n",
    "           valid_loader,\n",
    "           criterion,\n",
    "           optimizer,\n",
    "           lr_scheduler=lr_scheduler,\n",
    "           step='batch',\n",
    "           logger=None)"
   ],
   "id": "a1db414e89d7421c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "b9d3c68d2c979f9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "encoder = Encoder(model_name=CFG.model_name, pretrained=False, out_dim=256)\n",
    "decoder = Decoder(vocab_size=tokenizer.vocab_size,\n",
    "                encoder_length=CFG.num_patches, dim=256, num_heads=8, num_layers=6)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "model.to(CFG.device)\n",
    "\n",
    "msg = model.load_state_dict(torch.load('best_valid_loss.pth', map_location=CFG.device))\n",
    "print(msg)\n",
    "model.eval();"
   ],
   "id": "f3378f0b70392342",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "img_paths = ''\n",
    "img_paths = [path for path in img_paths.split(' ')]"
   ],
   "id": "c96c49d061d66a0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "img_paths",
   "id": "c3adb429af15e017",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from inference import VOCDatasetTest",
   "id": "addd866f30c7bfc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_dataset = VOCDatasetTest(img_paths, size=CFG.img_size)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=len(img_paths), shuffle=False, num_workers=0)"
   ],
   "id": "53912748fd4677d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import os\n",
    "from inference import generate, postprocess, visualize"
   ],
   "id": "f4f5e657f50212c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_bboxes = []\n",
    "all_labels = []\n",
    "all_confs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(test_loader):\n",
    "        batch_preds, batch_confs = generate(\n",
    "            model, x, tokenizer, max_len=CFG.generation_steps, top_k=0, top_p=1)\n",
    "        bboxes, labels, confs = postprocess(\n",
    "            batch_preds, batch_confs, tokenizer)\n",
    "        all_bboxes.extend(bboxes)\n",
    "        all_labels.extend(labels)\n",
    "        all_confs.extend(confs)\n",
    "\n",
    "os.mkdir(\"results\")\n",
    "for i, (bboxes, labels, confs) in enumerate(zip(all_bboxes, all_labels, all_confs)):\n",
    "    img_path = img_paths[i]\n",
    "    img = cv2.imread(img_path)[..., ::-1]\n",
    "    img = cv2.resize(img, (CFG.img_size, CFG.img_size))\n",
    "    img = visualize(img, bboxes, labels, id2cls, show=False)\n",
    "\n",
    "    cv2.imwrite(\"results/\" + img_path.split(\"/\")[-1], img[..., ::-1])"
   ],
   "id": "d0682f9fda9e4764",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6396031c3a25e9b8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
